# Week 1: Systems of linear equations

Linear algebra forms the mathematical foundation of machine learning and data science, providing the essential framework for representing and manipulating data efficiently. At its core, data is represented as vectors $\mathbf{x} \in \mathbb{R}^n$ and matrices $\mathbf{X} \in \mathbb{R}^{m \times n}$, where features become columns and observations become rows. Key operations like matrix multiplication $\mathbf{A}\mathbf{B}$, eigenvalue decomposition $\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$, and singular value decomposition $\mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$ enable dimensionality reduction techniques such as PCA. Neural networks fundamentally rely on linear transformations $\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}$ combined with non-linear activation functions, while optimization algorithms use gradients $\nabla f(\mathbf{x})$ to minimize loss functions. Understanding concepts like vector spaces, basis vectors, orthogonality, and matrix rank allows practitioners to grasp why algorithms work, debug implementations effectively, and develop intuition for high-dimensional data spaces where visualization becomes impossible.